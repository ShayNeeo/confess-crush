# README: HÃ nh TrÃ¬nh Khai PhÃ¡ Dá»¯ Liá»‡u KhÃ¡ch HÃ ng SÃ n TMÄT - Sá»© Má»‡nh DAZONE 2025 VÃ²ng 2.2


## Má»¥c Lá»¥c

1. [Nhiá»‡m Vá»¥ BÃ­ Máº­t: Hiá»ƒu LÃ²ng KhÃ¡ch HÃ ng, Giá»¯ ChÃ¢n Doanh Thu](#nhiá»‡m-vá»¥-bÃ­-máº­t-hiá»ƒu-lÃ²ng-khÃ¡ch-hÃ ng-giá»¯-chÃ¢n-doanh-thu)
2. [KÃ­ch Hoáº¡t "Cá»— MÃ¡y Thá»i Gian": HÆ°á»›ng Dáº«n Váº­n HÃ nh Script](#kÃ­ch-hoáº¡t-cá»—-mÃ¡y-thá»i-gian-hÆ°á»›ng-dáº«n-váº­n-hÃ nh-script)
3. [Báº£n Thiáº¿t Káº¿ "Cá»— MÃ¡y": Cáº¥u TrÃºc Script vÃ  CÃ¢u Chuyá»‡n Äáº±ng Sau](#báº£n-thiáº¿t-káº¿-cá»—-mÃ¡y-cáº¥u-trÃºc-script-vÃ -cÃ¢u-chuyá»‡n-Ä‘áº±ng-sau)
   * [Tá»•ng Quan Lá»™ TrÃ¬nh KhÃ¡m PhÃ¡](#tá»•ng-quan-lá»™-trÃ¬nh-khÃ¡m-phÃ¡)
   * [ChÆ°Æ¡ng 1: Thu Tháº­p ThÃ´ng Tin TÃ¬nh BÃ¡o - Chuáº©n Bá»‹ Dá»¯ Liá»‡u](#chÆ°Æ¡ng-1-thu-tháº­p-thÃ´ng-tin-tÃ¬nh-bÃ¡o---chuáº©n-bá»‹-dá»¯-liá»‡u)
   * [ChÆ°Æ¡ng 2: Cháº¿ Táº¡o Quáº£ Cáº§u TiÃªn Tri - XÃ¢y Dá»±ng vÃ  ÄÃ¡nh GiÃ¡ Model](#chÆ°Æ¡ng-2-cháº¿-táº¡o-quáº£-cáº§u-tiÃªn-tri---xÃ¢y-dá»±ng-vÃ -Ä‘Ã¡nh-giÃ¡-model)
   * [ChÆ°Æ¡ng 3: Äá»c Vá»‹ Quáº£ Cáº§u & PhÃ¢n Loáº¡i "Chiáº¿n Binh" - PhÃ¢n TÃ­ch SÃ¢u vÃ  Táº¡o ChÃ¢n Dung KhÃ¡ch HÃ ng](#chÆ°Æ¡ng-3-Ä‘á»c-vá»‹-quáº£-cáº§u--phÃ¢n-loáº¡i-chiáº¿n-binh---phÃ¢n-tÃ­ch-sÃ¢u-vÃ -táº¡o-chÃ¢n-dung-khÃ¡ch-hÃ ng)
4. [ÄÃ¡p Ãn Cho "Sáº¿p Tá»•ng": Äá»‘i Chiáº¿u Vá»›i Barem Cháº¥m Äiá»ƒm DAZONE 2025](#Ä‘Ã¡p-Ã¡n-cho-sáº¿p-tá»•ng-Ä‘á»‘i-chiáº¿u-vá»›i-barem-cháº¥m-Ä‘iá»ƒm-dazone-2025)
5. [BÃ¡o CÃ¡o Chiáº¿n CÃ´ng LÃªn "Bá»™ Chá»‰ Huy": Gá»£i Ã Trá»±c Quan HÃ³a Cho BÃ i Thuyáº¿t TrÃ¬nh](#bÃ¡o-cÃ¡o-chiáº¿n-cÃ´ng-lÃªn-bá»™-chá»‰-huy-gá»£i-Ã½-trá»±c-quan-hÃ³a-cho-bÃ i-thuyáº¿t-trÃ¬nh)
6. [ThÃ´ng Äiá»‡p Tá»« "Chá»‰ Huy TrÆ°á»Ÿng" Äá»™i Äáº·c Nhiá»‡m](#thÃ´ng-Ä‘iá»‡p-tá»«-chá»‰-huy-trÆ°á»Ÿng-Ä‘á»™i-Ä‘áº·c-nhiá»‡m)

## Nhiá»‡m Vá»¥ BÃ­ Máº­t: Hiá»ƒu LÃ²ng KhÃ¡ch HÃ ng, Giá»¯ ChÃ¢n Doanh Thu

SÃ n thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ cá»§a chÃºng ta cÃ³ hÃ ng triá»‡u khÃ¡ch hÃ ng. NhÆ°ng, ai trong sá»‘ há» sáº½ thá»±c sá»± gáº¯n bÃ³ vÃ  quay láº¡i mua hÃ ng táº¡i má»™t gian hÃ ng cá»¥ thá»ƒ trong 6 thÃ¡ng tá»›i? ÄÃ¢y khÃ´ng chá»‰ lÃ  má»™t cÃ¢u há»i, mÃ  lÃ  má»™t "nhiá»‡m vá»¥ tá»‘i máº­t" áº£nh hÆ°á»Ÿng trá»±c tiáº¿p Ä‘áº¿n doanh thu vÃ  sá»± phÃ¡t triá»ƒn bá»n vá»¯ng. Náº¿u chÃºng ta biáº¿t Ä‘Æ°á»£c Ä‘iá»u nÃ y, chÃºng ta cÃ³ thá»ƒ:

* ChÄƒm sÃ³c Ä‘Ãºng ngÆ°á»i, Ä‘Ãºng lÃºc.
* Tung ra cÃ¡c chÆ°Æ¡ng trÃ¬nh khuyáº¿n mÃ£i hiá»‡u quáº£ hÆ¡n.
* Biáº¿n khÃ¡ch hÃ ng má»›i thÃ nh khÃ¡ch hÃ ng trung thÃ nh.

"Cá»— mÃ¡y thá»i gian" (script Python nÃ y) sáº½ giÃºp chÃºng ta giáº£i quyáº¿t nhiá»‡m vá»¥ nÃ y báº±ng cÃ¡ch:

1. **"Thu tháº­p vÃ  lÃ m sáº¡ch cÃ¡c máº£nh ghÃ©p quÃ¡ khá»©"**: Chuáº©n bá»‹ dá»¯ liá»‡u giao dá»‹ch vÃ  thÃ´ng tin khÃ¡ch hÃ ng.
2. **"Cháº¿ táº¡o cÃ¡c phiÃªn báº£n quáº£ cáº§u tiÃªn tri"**: XÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n.
3. **"Kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c cá»§a tá»«ng quáº£ cáº§u"**: ÄÃ¡nh giÃ¡ model nÃ o "phÃ¡n" chuáº©n nháº¥t.
4. **"Nháº­n diá»‡n cÃ¡c nhÃ³m chiáº¿n binh mua sáº¯m"**: PhÃ¢n loáº¡i khÃ¡ch hÃ ng dá»±a trÃªn hÃ nh vi vÃ  Ä‘áº·c Ä‘iá»ƒm.
5. **"Váº¡ch ra káº¿ hoáº¡ch tÃ¡c chiáº¿n"**: Äá» xuáº¥t cÃ¡c chiáº¿n lÆ°á»£c kinh doanh thÃ´ng minh.

## KÃ­ch Hoáº¡t "Cá»— MÃ¡y Thá»i Gian": HÆ°á»›ng Dáº«n Váº­n HÃ nh Script

Äá»ƒ "cá»— mÃ¡y" cá»§a chÃºng ta báº¯t Ä‘áº§u hÃ nh trÃ¬nh xuyÃªn khÃ´ng vá» quÃ¡ khá»© vÃ  dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai, cáº£ Ä‘á»™i cáº§n chuáº©n bá»‹:

1. **"Trang bá»‹ cÃ¡ nhÃ¢n" (MÃ´i trÆ°á»ng):** MÃ¡y tÃ­nh Ä‘Æ°á»£c cÃ i Python vÃ  cÃ¡c "vÅ© khÃ­" cáº§n thiáº¿t (thÆ° viá»‡n) nhÆ° `pandas`, `numpy` (Ä‘á»ƒ xá»­ lÃ½ sá»‘ liá»‡u), `matplotlib`, `seaborn` (Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“), `scikit-learn` (bá»™ cÃ´ng cá»¥ xÃ¢y model), `imblearn` (Ä‘á»ƒ cÃ¢n báº±ng lá»±c lÆ°á»£ng cÃ¡c nhÃ³m khÃ¡ch hÃ ng), `xgboost`, `lightgbm` (2 "chiáº¿n mÃ£" máº¡nh máº½), vÃ  `shap` (kÃ­nh lÃºp soi tháº¥u model).

2. **"Báº£n Ä‘á»“ kho bÃ¡u cá»•" (Dá»¯ liá»‡u Ä‘áº§u vÃ o):** CÃ¡c file CSV Ä‘Ã£ Ä‘Æ°á»£c "lau chÃ¹i" tá»« vÃ²ng trÆ°á»›c hoáº·c tá»« script `data_cleaning.py`, cáº¥t giá»¯ trong `cleaning_results/cleaned_data/`:
   * `competition_train_features.csv`: ThÃ´ng tin huáº¥n luyá»‡n cÃ¡c "nhÃ  tiÃªn tri".
   * `competition_test_features.csv`: ThÃ´ng tin Ä‘á»ƒ "thá»­ tÃ i" cÃ¡c "nhÃ  tiÃªn tri".
   * `cleaned_user_info.csv`: Há»“ sÆ¡ chi tiáº¿t cá»§a tá»«ng khÃ¡ch hÃ ng.

3. **"Niá»‡m tháº§n chÃº" (Cháº¡y script):** Thá»±c thi file `DAZONE2025_R2.2_Main_Analysis.py`.

4. **"Chiáº¿n lá»£i pháº©m" (Káº¿t quáº£):** Má»i "bÃ­ máº­t" vÃ  "kho bÃ¡u" sáº½ Ä‘Æ°á»£c táº­p há»£p táº¡i thÆ° má»¥c `round_2.2`:
   * `model_outputs`: NÆ¡i cáº¥t giá»¯ cÃ¡c "quáº£ cáº§u tiÃªn tri" máº¡nh nháº¥t.
   * `segmentation_outputs`: "Há»“ sÆ¡ máº­t" cá»§a tá»«ng nhÃ³m khÃ¡ch hÃ ng.
   * `shap_outputs`: "Báº£n giáº£i mÃ£" cÃ¡ch "quáº£ cáº§u" Ä‘Æ°a ra dá»± Ä‘oÃ¡n.
   * `visualizations_from_main_analysis`: "Album áº£nh" ghi láº¡i nhá»¯ng khÃ¡m phÃ¡ quan trá»ng.
   * `logs`: "BiÃªn niÃªn sá»­" cá»§a cuá»™c hÃ nh trÃ¬nh.

## Báº£n Thiáº¿t Káº¿ "Cá»— MÃ¡y": Cáº¥u TrÃºc Script vÃ  CÃ¢u Chuyá»‡n Äáº±ng Sau

HÃ nh trÃ¬nh cá»§a chÃºng ta Ä‘Æ°á»£c chia thÃ nh 3 chÆ°Æ¡ng lá»›n, má»—i chÆ°Æ¡ng hÃ© lá»™ má»™t pháº§n cá»§a bá»©c tranh toÃ n cáº£nh vá» khÃ¡ch hÃ ng cá»§a sÃ n thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­.

### Tá»•ng Quan Lá»™ TrÃ¬nh KhÃ¡m PhÃ¡

ÄÃ¢y lÃ  bá»©c tranh toÃ n cáº£nh vá» hÃ nh trÃ¬nh cá»§a chÃºng ta, tá»« lÃºc báº¯t Ä‘áº§u vá»›i dá»¯ liá»‡u thÃ´ cho Ä‘áº¿n khi tÃ¬m ra nhá»¯ng "viÃªn ngá»c" insight:

```mermaid
graph TD
    A[Initial Data - CSV Features] --> PREPROC[Preprocessing - Scale, Encode, SMOTE for Initial Train];
    PREPROC --> OPT1_EVAL[Option 1: Train & Evaluate 4 Models on SMOTE Data];
    PREPROC --> OPT2_EVAL[Option 2: Train & Evaluate 6 Models on SMOTE Data];

    OPT1_EVAL --> SELECT_BEST_BASE[Select Best Base Model];
    OPT2_EVAL --> SELECT_BEST_BASE;

    SELECT_BEST_BASE -- Original Scaled Train Data --> RFECV_STEP[RFECV: Feature Selection for Best Base Model];
    RFECV_STEP --> SELECTED_FEATURES[Selected Feature Set];

    SELECTED_FEATURES -- Create X_train_smote_rfe, X_val_rfe --> TUNE_MODEL[Hyperparameter Tuning - RandomizedSearchCV for Best Base Model on X_train_rfe original];
    
    TUNE_MODEL --> FINAL_MODEL((Final Best Tuned Model));
    FINAL_MODEL -- Validation Data X_val_rfe --> EVAL_FINAL[Evaluate Final Tuned Model];

    FINAL_MODEL --> DEPLOY[Goal: Predict Next 6 Months];
    FINAL_MODEL -- Sample Data X_val_rfe or X_train_rfe --> SHAP_ANALYSIS{SHAP Analysis};
    SHAP_ANALYSIS --> IMPORTANT_FEATURES[Important Features via SHAP];

    %% Customer Segmentation Flow (Parallel or Post-Feature Engineering)
    DATA_FOR_SEG[Aggregated Customer Data + Demographics] --> PREPROC_SEG[Preprocessing for Segmentation];
    PREPROC_SEG --> OPTIMAL_K[Determine Optimal K - Elbow/Silhouette];
    OPTIMAL_K --> CLUSTERING[K-Means Clustering];
    CLUSTERING --> SEGMENTS[Customer Segments];
    SEGMENTS -- Combine with 'label' --> PROFILE_SEGMENTS[Evaluate & Profile Segments by Label];

    %% Styling (Optional)
    classDef data fill:#f9f,stroke:#333,stroke-width:2px;
    classDef process fill:#ccf,stroke:#333,stroke-width:2px;
    classDef model fill:#cfc,stroke:#333,stroke-width:2px,color:#000;
    classDef decision fill:#f80,stroke:#333,stroke-width:2px;
    classDef deployment fill:#e7d38f,stroke:#333,stroke-width:2px;
    classDef analysis fill:#d2b4de,stroke:#333,stroke-width:2px;

    class A,SELECTED_FEATURES,DATA_FOR_SEG,IMPORTANT_FEATURES,SEGMENTS,PROFILE_SEGMENTS data;
    class PREPROC,PREPROC_SEG,RFECV_STEP,OPTIMAL_K,CLUSTERING process;
    class OPT1_EVAL,OPT2_EVAL,SELECT_BEST_BASE,TUNE_MODEL,FINAL_MODEL,EVAL_FINAL model;
    class SHAP_ANALYSIS analysis;
    class DEPLOY deployment;
```
# DAZONE2025 R2.2 ML Analysis Workflow

## Comprehensive Machine Learning Pipeline for Customer Repurchase Prediction


graph TD
    %% Data Sources
    A1["ğŸ“Š competition_train_features.csv<br/><i>Training Features</i>"] --> SETUP
    A2["ğŸ“Š competition_test_features.csv<br/><i>Test Features</i>"] --> SETUP
    A3["ğŸ‘¥ cleaned_user_info.csv<br/><i>User Demographics</i>"] --> SETUP
    
    SETUP["ğŸ”§ Environment Setup<br/>Libraries & Config"] --> PART1

    subgraph PART1 ["ğŸ“ PART 1: Data Loading & Feature Preparation"]
        direction TB
        
        subgraph DATA_LOAD ["ğŸ“¥ Data Loading & Initial Processing"]
            LOAD_TRAIN["ğŸ“ˆ Load Training Data<br/>competition_train_features.csv"]
            LOAD_TEST["ğŸ“ˆ Load Test Data<br/>competition_test_features.csv"]
            LOAD_USER["ğŸ‘¥ Load User Info<br/>cleaned_user_info.csv"]
        end
        
        subgraph PREPROCESSING ["âš™ï¸ Data Preprocessing"]
            DROP_DATES["ğŸ—“ï¸ Drop Date Columns<br/>Non-predictive features"]
            ENCODE_CAT["ğŸ”¤ Categorical Encoding<br/>Label/One-hot encoding"]
            HANDLE_MISSING["â“ Handle Missing Values<br/>Imputation strategies"]
            FEATURE_ENG["ğŸ› ï¸ Feature Engineering<br/>Create derived features"]
        end
        
        LOAD_TRAIN --> DROP_DATES
        LOAD_TEST --> DROP_DATES
        DROP_DATES --> ENCODE_CAT
        ENCODE_CAT --> HANDLE_MISSING
        HANDLE_MISSING --> FEATURE_ENG
        
        FEATURE_ENG --> DF_ALIGNED["ğŸ“Š Aligned Datasets<br/><i>Train & Test Ready</i>"]
        DF_ALIGNED --> TRAIN_VAL_SPLIT["ğŸ”€ Train/Validation Split<br/><i>80/20 split</i>"]
        
        TRAIN_VAL_SPLIT --> X_TRAIN["ğŸ¯ X_train<br/><i>Training Features</i>"]
        TRAIN_VAL_SPLIT --> Y_TRAIN["ğŸ·ï¸ y_train<br/><i>Training Labels</i>"]
        TRAIN_VAL_SPLIT --> X_VAL["ğŸ¯ X_val<br/><i>Validation Features</i>"]
        TRAIN_VAL_SPLIT --> Y_VAL["ğŸ·ï¸ y_val<br/><i>Validation Labels</i>"]
        
        X_TRAIN --> SCALING["ğŸ“ Feature Scaling<br/>StandardScaler"]
        X_VAL --> SCALING
        SCALING --> SCALED_DATA["âœ¨ Scaled Features<br/><i>Normalized datasets</i>"]
    end

    subgraph PART2 ["ğŸ¤– PART 2: Model Training, Evaluation & Optimization"]
        direction TB
        
        SCALED_DATA --> SMOTE_BALANCE["ğŸ”„ SMOTE Balancing<br/>Address class imbalance"]
        SMOTE_BALANCE --> BALANCED_DATA["âš–ï¸ Balanced Training Set<br/><i>Equal class distribution</i>"]
        
        subgraph MODEL_EVAL ["ğŸ“Š Initial Model Evaluation"]
            direction TB
            OPTION1["ğŸ¯ Option 1: Base Models<br/>Random Forest, Logistic Regression<br/>Gradient Boosting, Decision Tree"]
            OPTION2["ğŸ¯ Option 2: Extended Models<br/>+ MLP, SVC, Extra Trees<br/>AdaBoost, Naive Bayes"]
            
            BALANCED_DATA --> OPTION1
            BALANCED_DATA --> OPTION2
            
            OPTION1 --> METRICS1["ğŸ“ˆ Performance Metrics 1<br/>Accuracy, Precision, Recall, F1"]
            OPTION2 --> METRICS2["ğŸ“ˆ Performance Metrics 2<br/>Extended evaluation"]
            
            METRICS1 --> MODEL_SELECTION["ğŸ† Best Model Selection<br/>Based on validation performance"]
            METRICS2 --> MODEL_SELECTION
        end
        
        subgraph FEATURE_SELECTION ["ğŸ¯ Feature Selection with RFECV"]
            direction TB
            MODEL_SELECTION --> RFECV["ğŸ” Recursive Feature Elimination<br/>Cross-Validation (RFECV)"]
            RFECV --> OPTIMAL_FEATURES["â­ Optimal Feature Set<br/><i>~19 selected features</i>"]
            
            SCALED_DATA --> FILTER_FEATURES["ğŸ›ï¸ Apply Feature Selection<br/>Filter to optimal features"]
            OPTIMAL_FEATURES --> FILTER_FEATURES
            FILTER_FEATURES --> REDUCED_DATA["ğŸ¯ Feature-Reduced Dataset<br/><i>Optimized for performance</i>"]
        end
        
        REDUCED_DATA --> FINAL_SMOTE["ğŸ”„ Final SMOTE Application<br/>On selected features"]
        FINAL_SMOTE --> FINAL_TRAINING_SET["âš–ï¸ Final Training Set<br/><i>Balanced + Feature-Selected</i>"]
        
        subgraph HYPERPARAMETER_TUNING ["âš¡ Hyperparameter Tuning"]
            direction TB
            MODEL_SELECTION --> SELECT_TOP2["ğŸ¯ Select Top 2 Models<br/>For intensive tuning"]
            SELECT_TOP2 --> GRID_SEARCH["ğŸ”§ Grid Search CV<br/>Hyperparameter optimization"]
            
            GRID_SEARCH --> TUNED_MODEL1["ğŸ”§ Tuned Model 1<br/><i>Optimized hyperparameters</i>"]
            GRID_SEARCH --> TUNED_MODEL2["ğŸ”§ Tuned Model 2<br/><i>Optimized hyperparameters</i>"]
            
            FINAL_TRAINING_SET --> FINAL_EVALUATION["ğŸ“Š Final Model Evaluation<br/>Cross-validation + holdout"]
            TUNED_MODEL1 --> FINAL_EVALUATION
            TUNED_MODEL2 --> FINAL_EVALUATION
            
            FINAL_EVALUATION --> CHAMPION_MODEL["ğŸ‘‘ Champion Model<br/><i>Best performing model</i>"]
        end
    end

    subgraph PART3 ["ğŸ” PART 3: SHAP Analysis, User Segmentation & Business Insights"]
        direction TB
        
        subgraph SHAP_ANALYSIS ["ğŸ’¡ SHAP Explainability Analysis"]
            direction LR
            CHAMPION_MODEL --> SHAP_CALC["ğŸ”¬ SHAP Value Calculation<br/>Feature importance analysis"]
            REDUCED_DATA --> SHAP_CALC
            
            SHAP_CALC --> SHAP_PLOTS["ğŸ“Š SHAP Visualizations<br/>Summary, waterfall, dependence plots"]
            SHAP_CALC --> FEATURE_IMPORTANCE["â­ Global Feature Importance<br/>Most predictive features"]
        end
        
        subgraph USER_SEGMENTATION ["ğŸ‘¥ User Segmentation & Clustering"]
            direction TB
            DF_ALIGNED --> USER_BEHAVIOR["ğŸ“Š Aggregate User Behavior<br/>Transaction patterns & metrics"]
            LOAD_USER --> USER_DEMOGRAPHICS["ğŸ‘¤ User Demographics<br/>Age, location, preferences"]
            
            USER_BEHAVIOR --> MERGE_DATA["ğŸ”— Merge Behavioral & Demographic<br/>Comprehensive user profiles"]
            USER_DEMOGRAPHICS --> MERGE_DATA
            
            MERGE_DATA --> SEGMENT_PREP["âš™ï¸ Segmentation Preprocessing<br/>Scaling & normalization"]
            SEGMENT_PREP --> ELBOW_METHOD["ğŸ“ˆ Optimal K Determination<br/>Elbow method analysis"]
            
            ELBOW_METHOD --> KMEANS["ğŸ¯ K-Means Clustering<br/><i>Typically K=4 segments</i>"]
            KMEANS --> USER_CLUSTERS["ğŸ‘¥ User Clusters<br/>Behavioral segments"]
            
            USER_CLUSTERS --> CLUSTER_PROFILES["ğŸ“Š Initial Cluster Profiling<br/>Segment characteristics"]
            
            DF_ALIGNED --> REPURCHASE_LABELS["ğŸ·ï¸ User-Level Repurchase Rates<br/>Aggregate purchase behavior"]
            
            CLUSTER_PROFILES --> MERGE_OUTCOMES["ğŸ”— Merge Clusters with Outcomes<br/>Segment + repurchase data"]
            REPURCHASE_LABELS --> MERGE_OUTCOMES
            
            MERGE_OUTCOMES --> ENHANCED_PROFILES["âœ¨ Enhanced Cluster Profiles<br/>Segments with repurchase rates"]
        end
        
        subgraph PERSONA_DEVELOPMENT ["ğŸ­ Persona Development"]
            direction TB
            ENHANCED_PROFILES --> PERSONA_RULES["ğŸ‘¤ Define Persona Criteria<br/>Thresholds & characteristics"]
            PERSONA_RULES --> ASSIGN_PERSONAS["ğŸ­ Assign User Personas<br/>Map clusters to personas"]
            
            ASSIGN_PERSONAS --> FINAL_PERSONAS["ğŸ­ Final User Personas<br/>Named & characterized segments"]
            FINAL_PERSONAS --> PERSONA_ANALYSIS["ğŸ“ˆ Persona Repurchase Analysis<br/>Conversion rates by persona"]
            
            ENHANCED_PROFILES --> VISUALIZATION["ğŸ“Š Cluster Visualizations<br/>2D/3D scatter plots"]
            PERSONA_ANALYSIS --> PERSONA_VIZ["ğŸ¨ Persona Visualizations<br/>Business-friendly charts"]
        end
    end

    subgraph BUSINESS_INSIGHTS ["ğŸš€ Business Impact & Recommendations"]
        direction TB
        CHAMPION_MODEL --> INSIGHTS_ENGINE["ğŸ’¡ Business Insights Generator<br/>Automated insight extraction"]
        FEATURE_IMPORTANCE --> INSIGHTS_ENGINE
        FINAL_PERSONAS --> INSIGHTS_ENGINE
        PERSONA_ANALYSIS --> INSIGHTS_ENGINE
        
        INSIGHTS_ENGINE --> KEY_INSIGHTS["ğŸ“‹ Key Business Insights<br/>Actionable recommendations"]
        INSIGHTS_ENGINE --> MARKETING_STRATEGY["ğŸ¯ Marketing Strategy<br/>Persona-based targeting"]
        INSIGHTS_ENGINE --> RETENTION_TACTICS["ğŸ”„ Retention Tactics<br/>Segment-specific approaches"]
        
        CHAMPION_MODEL --> DEPLOYMENT_PLAN["ğŸš€ Model Deployment Plan<br/>Production implementation"]
        CHAMPION_MODEL --> PREDICTION_PIPELINE["ğŸ”® Future Prediction Pipeline<br/>Real-time scoring"]
    end

    %% Styling with GitHub-compatible colors
    classDef dataSource fill:#e1f5fe,stroke:#0277bd,stroke-width:2px,color:#000
    classDef process fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000
    classDef model fill:#e8f5e8,stroke:#388e3c,stroke-width:2px,color:#000
    classDef decision fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#000
    classDef analysis fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#000
    classDef insights fill:#e0f2f1,stroke:#00695c,stroke-width:2px,color:#000
    classDef champion fill:#fff9c4,stroke:#f57f17,stroke-width:3px,color:#000,font-weight:bold
    classDef intermediate fill:#f5f5f5,stroke:#616161,stroke-width:1px,color:#000

    %% Apply classes
    class A1,A2,A3,LOAD_TRAIN,LOAD_TEST,LOAD_USER dataSource
    class DROP_DATES,ENCODE_CAT,HANDLE_MISSING,FEATURE_ENG,SCALING,SMOTE_BALANCE,FINAL_SMOTE,FILTER_FEATURES,SEGMENT_PREP process
    class OPTION1,OPTION2,RFECV,GRID_SEARCH,TUNED_MODEL1,TUNED_MODEL2,KMEANS,SHAP_CALC model
    class MODEL_SELECTION,SELECT_TOP2,ELBOW_METHOD,PERSONA_RULES,ASSIGN_PERSONAS decision
    class SHAP_PLOTS,FEATURE_IMPORTANCE,CLUSTER_PROFILES,ENHANCED_PROFILES,PERSONA_ANALYSIS,VISUALIZATION,PERSONA_VIZ analysis
    class KEY_INSIGHTS,MARKETING_STRATEGY,RETENTION_TACTICS,DEPLOYMENT_PLAN,PREDICTION_PIPELINE,INSIGHTS_ENGINE insights
    class CHAMPION_MODEL champion
    class SETUP,DF_ALIGNED,TRAIN_VAL_SPLIT,X_TRAIN,Y_TRAIN,X_VAL,Y_VAL,SCALED_DATA,BALANCED_DATA,METRICS1,METRICS2,OPTIMAL_FEATURES,REDUCED_DATA,FINAL_TRAINING_SET,FINAL_EVALUATION,USER_BEHAVIOR,USER_DEMOGRAPHICS,MERGE_DATA,USER_CLUSTERS,REPURCHASE_LABELS,MERGE_OUTCOMES,FINAL_PERSONAS intermediate

## ğŸ“‹ Workflow Summary

This comprehensive ML workflow diagram represents the **DAZONE2025 R2.2 Main Analysis** pipeline, structured in three main parts:

### ğŸ”§ **PART 1: Data Loading & Feature Preparation**
- **Data Sources**: Training features, test features, and user demographic data
- **Preprocessing**: Date removal, categorical encoding, missing value handling
- **Feature Engineering**: Creation of derived features for enhanced predictive power
- **Data Splitting**: Train/validation split with proper scaling and normalization

### ğŸ¤– **PART 2: Model Training, Evaluation & Optimization**
- **Initial Evaluation**: Comparison of 4-6 different ML algorithms
- **Feature Selection**: RFECV (Recursive Feature Elimination with Cross-Validation)
- **Class Balancing**: SMOTE technique to address imbalanced datasets
- **Hyperparameter Tuning**: Grid search optimization for top-performing models
- **Model Selection**: Champion model selection based on cross-validation performance

### ğŸ” **PART 3: SHAP Analysis, User Segmentation & Business Insights**
- **Explainability**: SHAP analysis for feature importance and model interpretability
- **User Segmentation**: K-means clustering for behavioral pattern identification
- **Persona Development**: Business-meaningful user personas with repurchase characteristics
- **Business Impact**: Actionable insights and deployment recommendations

## ğŸ¨ Visual Features

- **GitHub Compatible**: Uses solid colors instead of gradients for perfect GitHub display
- **Clear Hierarchy**: Distinguished node types with consistent color schemes
- **Professional Styling**: Clean borders, readable fonts, and logical flow
- **Comprehensive Coverage**: End-to-end ML pipeline from data to deployment

## ğŸš€ Usage

Copy the Mermaid code above and paste it into:
- **GitHub README files** (automatic rendering)
- **GitLab documentation** 
- **Notion pages**
- **Mermaid Live Editor**
- **VS Code with Mermaid extension**

Perfect for technical documentation, project presentations, and stakeholder communications!

